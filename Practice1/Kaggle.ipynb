{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3.8.8 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"interpreter":{"hash":"cdc1bd03f89181de8c68a6c3149f34a4fc8da3607cbc1d336dee1514f2a11f34"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":["The data set includes three iris species with 50 samples each as well as some properties about each flower.\n","\n","In this notebook I will explain How to effectively use logistic regression to solve classification problems. I will try to explain each and every step in a concise and clear manner. we will go through the following, step by step:\n","- [Reading and understanding the data](#read)\n","- [Data visualization and explanatory data analysis](#visual)\n","- [Feature engineering: Data prep for the model](#prepare)\n","- [Model building](#build)\n","- [Model evaluation](#eval1)\n","- [Model optimization: hyper parameter tuning](#hyper)\n","- [Model re-evaluation](#eval2)"],"metadata":{}},{"cell_type":"markdown","source":["### **<a id = \"read\">Reading and understanding the data</a>**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# import the necessary liberaries\r\n","import numpy as np # linear algebra\r\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\r\n","import matplotlib.pyplot as plt # visuals\r\n","import seaborn as sns # advanced visuals\r\n","\r\n","import warnings # ignore warnings\r\n","warnings.filterwarnings('ignore')"],"outputs":[],"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-01T16:23:04.753898Z","iopub.execute_input":"2021-08-01T16:23:04.754267Z","iopub.status.idle":"2021-08-01T16:23:04.759398Z","shell.execute_reply.started":"2021-08-01T16:23:04.754235Z","shell.execute_reply":"2021-08-01T16:23:04.758430Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# read the data\r\n","df = pd.read_csv(\"./practice1_data.csv\")"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:04.760861Z","iopub.execute_input":"2021-08-01T16:23:04.761175Z","iopub.status.idle":"2021-08-01T16:23:04.934208Z","shell.execute_reply.started":"2021-08-01T16:23:04.761125Z","shell.execute_reply":"2021-08-01T16:23:04.933183Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# display the first 5 rows\r\n","df.head()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:04.936131Z","iopub.execute_input":"2021-08-01T16:23:04.936488Z","iopub.status.idle":"2021-08-01T16:23:04.952798Z","shell.execute_reply.started":"2021-08-01T16:23:04.936456Z","shell.execute_reply":"2021-08-01T16:23:04.951730Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# main characteristics of the dataframe\r\n","df.info()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:04.954851Z","iopub.execute_input":"2021-08-01T16:23:04.955288Z","iopub.status.idle":"2021-08-01T16:23:04.971999Z","shell.execute_reply.started":"2021-08-01T16:23:04.955243Z","shell.execute_reply":"2021-08-01T16:23:04.970692Z"},"trusted":true}},{"cell_type":"markdown","source":["The dataframe has 150 non-null values. It has 6 variables, all of them are in the right data type.\n","the first variable \"Id\" seems to be redundant and unnecessary for the our analysis, we can drop it and keep the rest of variables. "],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# summary statistics\r\n","df.describe()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:04.999702Z","iopub.execute_input":"2021-08-01T16:23:05.000079Z","iopub.status.idle":"2021-08-01T16:23:05.037049Z","shell.execute_reply.started":"2021-08-01T16:23:05.000044Z","shell.execute_reply":"2021-08-01T16:23:05.035949Z"},"trusted":true}},{"cell_type":"markdown","source":["From the summary statistics we can notice that Sepal leafs are wider and longer than Petal lefs, this can be clearly demonstrated in the following image:\n","\n","![](https://www.integratedots.com/wp-content/uploads/2019/06/iris_petal-sepal-e1560211020463.png)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# How many species in our dataframe?\r\n","# is the data balanced?\r\n","df[\"classes\"].value_counts()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:05.038690Z","iopub.execute_input":"2021-08-01T16:23:05.039123Z","iopub.status.idle":"2021-08-01T16:23:05.050048Z","shell.execute_reply.started":"2021-08-01T16:23:05.039076Z","shell.execute_reply":"2021-08-01T16:23:05.048807Z"},"trusted":true}},{"cell_type":"markdown","source":["The data is clean and balanced with exactly the same number of flowers per species: 50 flowers. but why do we care about the balance between number of observations per class? \n","\n","Imbalanced classifications pose a challenge for predictive modeling as most of the machine learning algorithms used for classification were designed around the assumption of an equal number of examples for each class.\n","\n","For example, an imbalanced multiclass classification problem may have 80 percent examples in the first class, 18 percent in the second class, and 2 percent in a third class.\n","\n","The minority class is harder to predict because there are few examples of this class, by definition. This means it is more challenging for a model to learn the characteristics of examples from this class, and to differentiate examples from this class from the majority class (or classes).\n","\n","This is a problem because typically, the minority class is more important and therefore the problem is more sensitive to classification errors for the minority class than the majority class. \n","\n","More detailed explanation can be found [here](https://machinelearningmastery.com/what-is-imbalanced-classification/)."],"metadata":{"execution":{"iopub.status.busy":"2021-07-30T22:49:39.554606Z","iopub.execute_input":"2021-07-30T22:49:39.555077Z","iopub.status.idle":"2021-07-30T22:49:39.559541Z","shell.execute_reply.started":"2021-07-30T22:49:39.555049Z","shell.execute_reply":"2021-07-30T22:49:39.558718Z"}}},{"cell_type":"markdown","source":["### **<a id = 'visual'>Data visualization and explanatory data analysis</a>**"],"metadata":{}},{"cell_type":"markdown","source":["This section focuses on how to produce and analyze charts that meets the best practices in both academia and industry. we will try to meet the following criteria in each graph:\n","1. **Chose the right graph that suits the variable type:** to display the distribution of categorical variables we might opt for count or bar plot. As for continuous variables we might go with a histogram. If we wan to study the distribution of a continuous variable per each calss of other categorical variable we can use a box plots or a kde plot with hue parameter... etc.\n","2. **Maximize Dagt-Ink Ration:** it equals to the ink used to display the data devided by the total ink used in the graph. Try not to use so many colors without a good reason for that. Aviod using backround colors, or borders or any other unnecessary decorations.\n","3. **Use clear well written Titles, labels, and tick marks.**"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["fig, axes = plt.subplots(3, 2, figsize=(10,5), dpi = 100)\r\n","fig.suptitle('Distribution of (x3, x7, x8, x9, x10)')\r\n","\r\n","# Distribution of sepal length per classes\r\n","sns.kdeplot(ax = axes[0,0], data = df, x = 'x3', hue = \"classes\", alpha = 0.5, shade = True)\r\n","axes[0,0].set_xlabel(\"x3\")\r\n","axes[0,0].get_legend().remove()\r\n","\r\n","# Distribution of sepal width per classes\r\n","sns.kdeplot(ax = axes[0,1], data = df, x = 'x7', hue = \"classes\", alpha = 0.5, shade = True)\r\n","axes[0,1].set_xlabel(\"x7\")\r\n","axes[0,1].get_legend().remove()\r\n","\r\n","# Distribution of petal length per classes\r\n","sns.kdeplot(ax = axes[1,0], data = df, x = 'x8', hue = \"classes\", alpha = 0.5, shade = True)\r\n","axes[1,0].set_xlabel(\"x8\")\r\n","axes[1,0].get_legend().remove()\r\n","\r\n","# Distribution of petal width per classes\r\n","sns.kdeplot(ax = axes[1,1], data = df, x = 'x9', hue = \"classes\", alpha = 0.5, shade = True)\r\n","axes[1,1].set_xlabel(\"x9\")\r\n","\r\n","# Distribution of petal width per classes\r\n","sns.kdeplot(ax = axes[2,0], data = df, x = 'x10', hue = \"classes\", alpha = 0.5, shade = True)\r\n","axes[1,1].set_xlabel(\"x10\")\r\n","\r\n","plt.tight_layout()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:05.053557Z","iopub.execute_input":"2021-08-01T16:23:05.053926Z","iopub.status.idle":"2021-08-01T16:23:06.032223Z","shell.execute_reply.started":"2021-08-01T16:23:05.053894Z","shell.execute_reply":"2021-08-01T16:23:06.031008Z"},"trusted":true}},{"cell_type":"markdown","source":["**Main conclusions from the graph:**\n","1. Setosa is easily separable from the other species, this means that the model will be able to classify it accurately.\n","2. Petal length and width is expected to be  better predictors of Species than Sepal lenght and width.\n","\n","Both conclusions can be demonstrated in the following picture where Setosa is clearly different from other sepcies especially when it comes to its petal leefs, it has a very small sepal width and length comapred to other species.\n","\n","![](https://miro.medium.com/max/900/0*Uw37vrrKzeEWahdB)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Scatter plot od petal length vs petal width\r\n","plt.figure(figsize = (7, 3), dpi = 100)\r\n","sns.scatterplot(data = df, x = 'x3', y = 'x7', hue = \"classes\")\r\n","plt.title(\"Classes x3 and x7\")\r\n","plt.xlabel(\"x3\")\r\n","plt.ylabel(\"x7\")\r\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\r\n","plt.show()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:06.034754Z","iopub.execute_input":"2021-08-01T16:23:06.035211Z","iopub.status.idle":"2021-08-01T16:23:06.319275Z","shell.execute_reply.started":"2021-08-01T16:23:06.035148Z","shell.execute_reply":"2021-08-01T16:23:06.318085Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# Scatter plot od sepal length vs petal width\r\n","plt.figure(figsize = (7, 3), dpi = 100)\r\n","sns.scatterplot(data = df, x = 'x8', y = 'x9', hue = \"classes\")\r\n","plt.title(\"Classes for x8 x9\")\r\n","plt.xlabel(\"x8\")\r\n","plt.ylabel(\"x9\")\r\n","plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\r\n","plt.show()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:06.320672Z","iopub.execute_input":"2021-08-01T16:23:06.320982Z","iopub.status.idle":"2021-08-01T16:23:06.617382Z","shell.execute_reply.started":"2021-08-01T16:23:06.320942Z","shell.execute_reply":"2021-08-01T16:23:06.616413Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["#box plots\r\n","fig, axes = plt.subplots(3, 2, figsize=(10,5), dpi = 100)\r\n","\r\n","#x3\r\n","sns.boxplot(ax = axes[0,0], data = df, x = \"classes\", y = 'x3')\r\n","axes[0,0].set_xlabel(None)\r\n","axes[0,0].set_ylabel(None)\r\n","axes[0,0].set_title(\"x3\")\r\n","\r\n","\r\n","#x7\r\n","sns.boxplot(ax = axes[0,1], data = df, x = \"classes\", y = 'x7')\r\n","axes[0,1].set_xlabel(None)\r\n","axes[0,1].set_ylabel(None)\r\n","axes[0,1].set_title(\"x7\")\r\n","\r\n","#x8\r\n","sns.boxplot(ax = axes[1,0], data = df, x = \"classes\", y = 'x8')\r\n","axes[1,0].set_xlabel(None)\r\n","axes[1,0].set_ylabel(None)\r\n","axes[1,0].set_title(\"x8\")\r\n","\r\n","#x9\r\n","sns.boxplot(ax = axes[1,1], data = df, x = \"classes\", y = 'x9')\r\n","axes[1,1].set_xlabel(None)\r\n","axes[1,1].set_ylabel(None)\r\n","axes[1,1].set_title(\"x9\")\r\n","\r\n","#x10\r\n","sns.boxplot(ax = axes[2,0], data = df, x = \"classes\", y = 'x10')\r\n","axes[1,1].set_xlabel(None)\r\n","axes[1,1].set_ylabel(None)\r\n","axes[1,1].set_title(\"x10\")\r\n","\r\n","plt.tight_layout()\r\n","plt.subplots_adjust(hspace=0.5)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:06.618822Z","iopub.execute_input":"2021-08-01T16:23:06.619156Z","iopub.status.idle":"2021-08-01T16:23:07.428055Z","shell.execute_reply.started":"2021-08-01T16:23:06.619125Z","shell.execute_reply":"2021-08-01T16:23:07.426976Z"},"trusted":true}},{"cell_type":"markdown","source":["Scatter and box plots confirmed the aforementioned conclusion, setosa is easily separable based on petal length and width."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Correlation map\r\n","plt.figure(figsize = (8, 4), dpi = 100)\r\n","sns.heatmap(df.corr(), annot = True, cmap = \"viridis\", vmin = -1, vmax = 1)\r\n","plt.title(\"Correlation map between variables\")\r\n","#plt.xticks(rotation = 90)\r\n","plt.show()"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.429602Z","iopub.execute_input":"2021-08-01T16:23:07.429931Z","iopub.status.idle":"2021-08-01T16:23:07.801501Z","shell.execute_reply.started":"2021-08-01T16:23:07.429896Z","shell.execute_reply":"2021-08-01T16:23:07.800414Z"},"trusted":true}},{"cell_type":"markdown","source":["[Correlation is](https://www.jmp.com/en_in/statistics-knowledge-portal/what-is-correlation.html) a statistical measure that expresses the extent to which two variables are linearly related (meaning they change together at a constant rate). It's a common tool for describing simple relationships without making a statement about cause and effect.\n","\n","Correlation coefficient ranges between -1 (perfect negative correlation) and 1 (perfect positive correlation). As you can notice, there is a strong positive correlation between petal width and length on one hand and sepal length on the other hand. "],"metadata":{}},{"cell_type":"markdown","source":["### **<a id = \"prepare\">Feature engineering: Data prep for the model</a>**"],"metadata":{}},{"cell_type":"markdown","source":["In this section we will make sure that the data is well prepared for training the model. We will:\n","\n","1. Seprate the dependent variable from the independent ones. \n","2. Perform a train test split \n","3. Scale the data (feature scaling)."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# 1. Seprate the dependent variable from the independent ones.\r\n","\r\n","X = df.drop(\"classes\", axis = 1)\r\n","y = df[\"classes\"]"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.802711Z","iopub.execute_input":"2021-08-01T16:23:07.803014Z","iopub.status.idle":"2021-08-01T16:23:07.808977Z","shell.execute_reply.started":"2021-08-01T16:23:07.802984Z","shell.execute_reply":"2021-08-01T16:23:07.807571Z"},"trusted":true}},{"cell_type":"markdown","source":["**Why train test split ?** we need to split the data into two parts: \n","1. Training part, we will use it to train the model.\n","2. Test part: this is unseen data (the model has never seen it before), we will use it the test the real performance of the model.\n","\n","**Why we need to test on unseen data?** why we do not simply train the model on the whole data and then reuse some of it for evaluation? because this will be like giving the student the answers before entring the exam, the model will be very familiar with the evaluation data because he has seen them before and he will get a full mark. In order for the test to be real, the model has to be evaluated on unseen data."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# 2. Perform a train test split\r\n","from sklearn.model_selection import train_test_split\r\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.810731Z","iopub.execute_input":"2021-08-01T16:23:07.811272Z","iopub.status.idle":"2021-08-01T16:23:07.826429Z","shell.execute_reply.started":"2021-08-01T16:23:07.811226Z","shell.execute_reply":"2021-08-01T16:23:07.825495Z"},"trusted":true}},{"cell_type":"markdown","source":["**Why feature scaling?** \n","\n","Real Life Datasets have many features with a wide range of values like for example let’s consider the house price prediction dataset. It will have many features like no. of. bedrooms, square feet area of the house, etc\n","\n","As you can guess, the no. of bedrooms will vary between 1 and 5, but the square feet area will range from 500-2000. This is a huge difference in the range of both features.\n","\n","Many machine learning algorithms that are using Euclidean distance as a metric to calculate the similarities will fail to give a reasonable recognition to the smaller feature, in this case, the number of bedrooms, which in the real case can turn out to be an actually important metric.\n","\n","![](https://i.imgflip.com/2rcqrd.png)\n","\n","To aviod this problem we need to scale the features so that they all have the same scale, i.e the same range of values. We can normalize all features so that have values between (-1, 1) or standardize them to have values between (0, 1). \n","\n","The important thing to note here is that feature scaling does not affect the relative importance of features, scaled features will still have the same orginal information and importance relative to each other, this can be clearly demonstated from the image below: despite feature scaling they are still strawberry and apple, they did not lose their meaning.\n","\n","![](https://miro.medium.com/max/2000/1*yR54MSI1jjnf2QeGtt57PA.png)"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# 3. Feature scaling\r\n","from sklearn.preprocessing import StandardScaler # import the scaler\r\n","scaler = StandardScaler() # initiate it\r\n","Scaled_X_train = scaler.fit_transform(X_train) #fit the parameters and use it to trannsform the traning data\r\n","Scaled_X_test = scaler.transform(X_test) #transform the test data"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.828058Z","iopub.execute_input":"2021-08-01T16:23:07.828682Z","iopub.status.idle":"2021-08-01T16:23:07.844982Z","shell.execute_reply.started":"2021-08-01T16:23:07.828631Z","shell.execute_reply":"2021-08-01T16:23:07.843908Z"},"trusted":true}},{"cell_type":"markdown","source":["Have you noticed that we used .fit_transform() with the traning data and only used .transform() with the test data? we did it to aviod data leakage. Read more about it [from here](https://machinelearningmastery.com/data-preparation-without-data-leakage/) "],"metadata":{}},{"cell_type":"markdown","source":["### **<a id = \"build\">Model building</a>** \n","We will use logestic regression, but the same methodology can be applied to any other classifier"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# Logestic Regression \r\n","from sklearn.linear_model import LogisticRegression # import the classifier\r\n","log_model = LogisticRegression() #initiate it\r\n","log_model.fit(Scaled_X_train, y_train) #fit the model to the training data"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.847024Z","iopub.execute_input":"2021-08-01T16:23:07.848175Z","iopub.status.idle":"2021-08-01T16:23:07.873329Z","shell.execute_reply.started":"2021-08-01T16:23:07.848141Z","shell.execute_reply":"2021-08-01T16:23:07.872317Z"},"trusted":true}},{"cell_type":"markdown","source":["### **<a id = \"eval1\">Model evaluation</a>**\n","First we will make predictions using the model on the test data, and then evaluate its performance using the following metrics:\n","1. **Confusion matrix:** A summary of prediction results on a classification problem. The number of correct and incorrect predictions are summarized with count values and broken down by each class. Here is an example of a confusion matrix: \n","\n","![](https://cdn.analyticsvidhya.com/wp-content/uploads/2020/04/Example-Confusion-matrix.png)\n","\n","2. **Accuracy score:** the fraction of predictions our model got right (number of correct predictions devided by total number of predictions).\n","3. **Classification report:** used to measure the quality of predictions from a classification algorithm. How many predictions are True and how many are False. The report shows the main classification metrics precision, recall and f1-score on a per-class basis. **Precision:** What percent of your predictions were correct? - **Recall:** What percent of the positive cases did you catch? - **F1 score:** What percent of positive predictions were correct?.\n","\n","For more info, click [here](https://scikit-learn.org/stable/modules/model_evaluation.html#model-evaluation) and [here](https://muthu.co/understanding-the-classification-report-in-sklearn/)."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# creating predictions \r\n","y_pred = log_model.predict(Scaled_X_test)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.874884Z","iopub.execute_input":"2021-08-01T16:23:07.875355Z","iopub.status.idle":"2021-08-01T16:23:07.882221Z","shell.execute_reply.started":"2021-08-01T16:23:07.875306Z","shell.execute_reply":"2021-08-01T16:23:07.880804Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# import evaluation metrics \r\n","from sklearn.metrics import accuracy_score, confusion_matrix, plot_confusion_matrix, classification_report"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.883842Z","iopub.execute_input":"2021-08-01T16:23:07.884377Z","iopub.status.idle":"2021-08-01T16:23:07.896005Z","shell.execute_reply.started":"2021-08-01T16:23:07.884323Z","shell.execute_reply":"2021-08-01T16:23:07.895041Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# create the confusion matrix\r\n","confusion_matrix(y_test, y_pred)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.897474Z","iopub.execute_input":"2021-08-01T16:23:07.897808Z","iopub.status.idle":"2021-08-01T16:23:07.918854Z","shell.execute_reply.started":"2021-08-01T16:23:07.897769Z","shell.execute_reply":"2021-08-01T16:23:07.917943Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# plot the confusion matrix\r\n","fig, ax = plt.subplots(dpi = 120)\r\n","plot_confusion_matrix(log_model, Scaled_X_test, y_test, ax = ax);"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:07.920168Z","iopub.execute_input":"2021-08-01T16:23:07.920754Z","iopub.status.idle":"2021-08-01T16:23:08.213090Z","shell.execute_reply.started":"2021-08-01T16:23:07.920711Z","shell.execute_reply":"2021-08-01T16:23:08.212306Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# measure the accuracy of our model\r\n","acc_score = accuracy_score(y_test, y_pred)\r\n","round(acc_score, 2)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:23:08.214371Z","iopub.execute_input":"2021-08-01T16:23:08.214954Z","iopub.status.idle":"2021-08-01T16:23:08.223717Z","shell.execute_reply.started":"2021-08-01T16:23:08.214910Z","shell.execute_reply":"2021-08-01T16:23:08.222778Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# generate the classification report \r\n","print(classification_report(y_test, y_pred)) # Hint: try it without using the print() method"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:25:18.757236Z","iopub.execute_input":"2021-08-01T16:25:18.757850Z","iopub.status.idle":"2021-08-01T16:25:18.771451Z","shell.execute_reply.started":"2021-08-01T16:25:18.757798Z","shell.execute_reply":"2021-08-01T16:25:18.770069Z"},"trusted":true}},{"cell_type":"markdown","source":["As we expected before, the model did a perfect job predicting Setosa. It only misclassified one observation as versicolor, where in fact it is virginica. However, the model performance is near perfect and we could not have done better than that."],"metadata":{}},{"cell_type":"markdown","source":["### **<a id = \"hyper\">Model optimization: hyper parameter tuning</a>**\n","Hyperparameter tuning [is](https://neptune.ai/blog/hyperparameter-tuning-in-python-a-complete-guide-2020) the process of determining the right combination of parameters that allows us to maximize model performance. We will try different values for each parameter and choose the ones that give us the best predictions."],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# import GridSearchCV\r\n","from sklearn.model_selection import GridSearchCV \r\n","\r\n","# set the range of paprameters\r\n","penalty = ['l1', 'l2', 'elasticnet']\r\n","C = np.logspace(0,20,50)\r\n","solver = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\r\n","multi_class = ['ovr', 'multinomial']\r\n","l1_ratio = np.linspace(0, 1, 20)\r\n","\r\n","# build the parameter grid\r\n","param_grid = {\r\n","   'penalty': penalty,\r\n","    'C': C,\r\n","    'solver': solver,\r\n","    'multi_class': multi_class, \r\n","    'l1_ratio': l1_ratio\r\n","}\r\n","\r\n","# initiate and fit the Grid Search Model\r\n","grid_model = GridSearchCV(log_model, param_grid = param_grid)\r\n","grid_model.fit(Scaled_X_train, y_train)\r\n"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:39:30.847392Z","iopub.execute_input":"2021-08-01T16:39:30.847762Z","iopub.status.idle":"2021-08-01T16:52:24.347094Z","shell.execute_reply.started":"2021-08-01T16:39:30.847733Z","shell.execute_reply":"2021-08-01T16:52:24.346027Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# best parameters \r\n","grid_model.best_params_"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:53:25.651674Z","iopub.execute_input":"2021-08-01T16:53:25.652054Z","iopub.status.idle":"2021-08-01T16:53:25.657852Z","shell.execute_reply.started":"2021-08-01T16:53:25.652017Z","shell.execute_reply":"2021-08-01T16:53:25.656784Z"},"trusted":true}},{"cell_type":"markdown","source":["### **<a id = \"eval2\">Model re-evaluation</a>**\n","We will evaluate the optimized version of our model and see if it does better than the base model"],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# creating predictions \r\n","y_pred = grid_model.predict(Scaled_X_test)\r\n","\r\n","# plot the confusion matrix\r\n","fig, ax = plt.subplots(dpi = 120)\r\n","plot_confusion_matrix(grid_model, Scaled_X_test, y_test, ax = ax);"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:56:00.905253Z","iopub.execute_input":"2021-08-01T16:56:00.905651Z","iopub.status.idle":"2021-08-01T16:56:01.180918Z","shell.execute_reply.started":"2021-08-01T16:56:00.905618Z","shell.execute_reply":"2021-08-01T16:56:01.179830Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# measure the accuracy of our model\r\n","acc_score = accuracy_score(y_test, y_pred)\r\n","round(acc_score, 2)"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:56:22.969913Z","iopub.execute_input":"2021-08-01T16:56:22.970411Z","iopub.status.idle":"2021-08-01T16:56:22.977730Z","shell.execute_reply.started":"2021-08-01T16:56:22.970363Z","shell.execute_reply":"2021-08-01T16:56:22.976739Z"},"trusted":true}},{"cell_type":"code","execution_count":null,"source":["# generate the classification report \r\n","print(classification_report(y_test, y_pred)) # Hint: try it without using the print() method"],"outputs":[],"metadata":{"execution":{"iopub.status.busy":"2021-08-01T16:56:38.238479Z","iopub.execute_input":"2021-08-01T16:56:38.238877Z","iopub.status.idle":"2021-08-01T16:56:38.253848Z","shell.execute_reply.started":"2021-08-01T16:56:38.238839Z","shell.execute_reply":"2021-08-01T16:56:38.252395Z"},"trusted":true}},{"cell_type":"markdown","source":["The optimized model did a completely perfect job. I correctly classified all the examples in the test data. The accuracy of the model is 100 percent. Accuracy improved from 97 percent for the base model to 100 percent for the optimized model."],"metadata":{}},{"cell_type":"markdown","source":["Congratulations! you have made it to the end of the tutorial. Please leave your feedback and suggestions of improvement"],"metadata":{}}]}